
## 第1章　はじめに：プロジェクトの背景と目的

### 1.1 背景：大規模AIワークロードにおけるVRAMの課題

近年、大規模言語モデル（LLM）を中心としたAI技術の急速な発展に伴い、GPUのビデオメモリ（VRAM）に対する要求は爆発的に増大している。特に推論（Inference）や学習（Training）の現場において、VRAM は最も高価かつ希少なリソースであり、その利用効率がシステムの性能とコストを直接左右する。

しかし、現代のAIワークロード、とりわけ **KVキャッシュ（Key-Value Cache）** を伴う長文脈（Long Context）推論においては、以下の構造的なメモリ課題が顕在化している。

#### 1. 動的なメモリ消費と予測の困難さ
LLMの推論プロセスでは、生成されるトークン数に応じて KV キャッシュが動的に増大する。
ユーザーからの入力長や生成される出力長は実行時まで確定しないため、事前に必要なメモリ量を静的に確保することは不可能である。結果として、アプリケーションは推論の進行に合わせて頻繁にメモリの確保（Allocation）と解放（Deallocation）を繰り返す必要がある。

#### 2. 物理メモリの断片化（フラグメンテーション）
頻繁な確保と解放は、VRAM 空間上に「使用中」と「空き」がモザイク状に混在する **断片化（Fragmentation）** を引き起こす。
従来のメモリアロケータは、物理的に連続した領域を要求するため、断片化が進行すると以下のパラドックスが発生する。

* **現象**: VRAM の合計空き容量は十分にある（例: 10GB空いている）。
* **結果**: しかし、連続した 2GB の領域が見つからず、**OOM (Out Of Memory)** でクラッシュする。

この「見せかけのメモリ不足」により、高価な GPU リソースの実効利用率が 50〜60% 程度に留まってしまうケースも珍しくない。

#### 3. リサイズコストによる性能低下
断片化を回避、あるいは容量不足を解消するために、既存のバッファをより大きな領域へ移し替える「リサイズ（Realloc相当）」処理が必要となる。

しかし、GPU 上でのデータ移動（Memcpy）は、広帯域幅を持つ H100 等の最新 GPU であっても無視できないレイテンシ（数ミリ秒〜数百ミリ秒）を発生させる。

これが推論のレイテンシ（Time to First Token / Inter-Token Latency）を悪化させ、ユーザー体験を損なう主因となっている。

以上のことから、物理メモリの制約に縛られず、かつデータ移動コストを支払わない、**新しい次元のメモリ管理手法** が強く求められているのである。

### 1.2 既存技術（Runtime API）の構造的限界

CUDA プログラミングにおいて標準的に使用される Runtime API (`cudaMalloc`, `cudaMemcpy`, `cudaFree`) は、静的なメモリ確保を前提とした古い設計思想に基づいている。動的なリサイズや複雑なメモリ管理が求められる現代のワークロードに対し、この既存技術は以下の3つの構造的な限界を抱えている。

#### 1. 物理メモリと仮想アドレスの不可分な結合
`cudaMalloc` によって確保されるポインタは、**「仮想アドレス（Virtual Address）」と「物理メモリ（Physical Memory）」が 1対1 で強固に結合されている。**
この結合は確保から解放まで不変であり、以下のような柔軟性を欠く結果を招く。

* **移動の不可能性**: 仮想アドレスを変えずに物理メモリだけを差し替えることや、逆に物理メモリを維持したまま仮想アドレスを変更することができない。
* **不連続の不許容**: 物理的に連続したメモリ領域が確保できなければ、たとえ合計空き容量が十分でも割り当ては失敗する。OS のページング機構のような柔軟なマッピング操作が、アプリケーション層からは一切行えない。

#### 2. リサイズ時の「時間」と「空間」の二重コスト
CUDA Runtime API には、標準 C ライブラリの `realloc` に相当する機能が存在しない。そのため、既存のバッファを拡張（リサイズ）するには、以下の手順を踏む必要がある。

1.  **新規確保**: 拡張後のサイズで新しい領域 (`ptr_new`) を確保する。
2.  **データコピー**: 古い領域 (`ptr_old`) から新しい領域へ、全データを `cudaMemcpy` で転送する。
3.  **解放**: 古い領域を `cudaFree` する。

このプロセスは、以下の二重のコストをシステムに強いる。

* **時間的コスト ($O(N)$)**:
    データ量 $N$ に比例したコピー時間が発生する。GB 単位のデータ転送は GPU のメモリ帯域を占有し、計算パイプラインを停止させる致命的なレイテンシとなる。
* **空間的コスト (メモリ倍増)**:
    コピーが完了するまでの間、VRAM 上には「移動元」と「移動先」の両方が同時に存在しなければならない。つまり、リサイズ操作の一瞬だけ、**必要メモリ量が一時的に2倍になる**。
    VRAM が既に逼迫している状況下では、この「一時的な倍増」が許容できず、リサイズそのものが OOM で失敗する原因となる。

#### 3. 断片化に対する無力さ
`cudaMalloc` は「連続した領域」を要求するため、繰り返される確保と解放によって VRAM が断片化（フラグメンテーション）した場合、隙間を埋める手段を持たない。
Driver API レベルではページ単位の管理が可能であっても、Runtime API の抽象化層がその操作を隠蔽しているため、アプリケーションは「物理メモリの再配置（デフラグメンテーション）」や「不連続ページの結合」といった最適化を行う術を持たないのである。

以上の限界により、Runtime API への依存は、大規模かつ動的な AI アプリケーションにおいて、性能と安定性の双方を損なうボトルネックとなっている。

### 1.3 解決策：Driver API VMM によるメモリ仮想化

Runtime API の抱える構造的限界を打破するために、本プロジェクトでは CUDA 10.2 以降で導入された低レイヤ機能である **Virtual Memory Management (VMM)** を採用する。
VMM は、GPU メモリ管理において OS のページング機構と同等の柔軟性を提供するものであり、その核心は以下の2つの概念にある。

#### 1. 物理メモリと仮想アドレスの分離 (Decoupling)
VMM においては、メモリの確保プロセスが以下の3段階に明確に分離されている。

1.  **Reserve (仮想アドレス予約)**:
    物理メモリを消費せず、仮想アドレス空間（VA）のみを確保する。64bit 空間の広大さを利用し、テラバイト級の領域を予約することも可能である。
2.  **Create (物理メモリ作成)**:
    仮想アドレスとは無関係に、物理メモリハンドル（PA）のみを作成する。
3.  **Map (マッピング)**:
    任意の VA に対して、任意の PA を紐付ける。

この分離により、**「アドレスを変えずに中身（物理メモリ）だけを入れ替える」** 操作や、**「物理メモリを消費せずに将来の拡張領域を確保しておく」** といった、Runtime API では不可能な制御が可能となる。

#### 2. スキャッター・ギャザー (Scatter-Gather) 機構
断片化に対する決定的な解決策が、不連続な物理メモリを連続した仮想アドレスにマッピングする **スキャッター・ギャザー** である。

* **物理的な実態**: VRAM 上に小さな空き領域（断片）が散乱している状態。
* **仮想的な見え方**: それらの断片をかき集め、仮想アドレス空間上で隙間なく並べることで、アプリケーションには「巨大な連続したバッファ」として認識させる。

これにより、物理的な断片化状況に関わらず、VRAM の空き容量の合計が要求サイズを満たす限り、割り当てを成功させることができる。

#### 3. ゼロコピー・リサイズ (Zero-Copy Resize)
上記2つの特性を組み合わせることで、本ライブラリの目的である「高速リサイズ」が実現される。
バッファの拡張が必要になった際、データの物理的なコピー（`Memcpy`）を行う必要はない。

* **手順**: 新たに必要な分だけ物理メモリを追加確保し、予約済みの仮想アドレスの末尾に `Map` する。
* **効果**: 既存データは 1bit も動かすことなく、瞬時に（定数時間で）バッファサイズが増大する。

Driver API VMM は、単なるメモリ確保の手段ではなく、物理的な制約（断片化・帯域幅）を論理層で完全に隠蔽するための仮想化技術である。

本ライブラリはこの機構を駆使し、アプリケーションに対して「無限に連続し、瞬時に伸縮自在なメモリ空間」の幻想を提供する。

### 1.4 設計理念：完全透過性とゼロコピー

本プロジェクトのアーキテクチャは、単なる機能の実装ではなく、以下の3つの譲れない設計理念に基づいて構築されている。

#### 1. LD_PRELOAD による完全透過性 (Full Transparency)
現代の AI/HPC 環境において、アプリケーションのソースコードを修正・再コンパイルすることは、以下の理由により現実的ではない場合が多い。

* **ブラックボックス化されたバイナリ**: 商用アプリケーションや、著作権・セキュリティ保護のためにソースコードが開示されていないプロプライエタリな推論エンジン。
* **極めて複雑なビルド依存関係**: PyTorch や TensorFlow などの巨大なフレームワーク内部のメモリ管理ルーチンに手を入れることは、メンテナンスコストや互換性の観点から許容されない。
* **レガシー資産の運用**: 既に開発が終了し、バイナリのみが稼働しているシステム。

これらの制約を突破する唯一の手段が、Linux の動的リンカ機能である `LD_PRELOAD` の活用である。
本ライブラリは、アプリケーション起動時に `cudaMalloc` や `cudaMemcpy` といった標準シンボルをフックし、その呼び出しを透過的に VMM マネージャーへとリダイレクトする。

これにより、**既存のバイナリに一切手を加えることなく（Zero Modification）**、外部から最新のメモリ管理ロジックを注入することを可能にする。この「非侵襲性」こそが、本ライブラリの実用性を担保する最大の要件である。

#### 2. 徹底したゼロコピー (Zero-Copy)
データ移動は、計算リソースの浪費である。本ライブラリは、メモリリサイズ時における「データの物理的移動」を徹底的に排除する。

* **物理の否定**: データの価値は「中身」にあり、「場所（物理アドレス）」にはない。VMM を用いて物理メモリのマッピング先を変更することで、データの実体を 1bit も動かすことなく、アプリケーションから見たアドレス空間のみを操作する。
* **定数時間 ($O(1)$) の保証**: データサイズが 1GB であろうと 100GB であろうと、マッピングの変更にかかる時間は一定（数マイクロ秒オーダー）である。これにより、データ規模に比例してレイテンシが悪化する従来の呪縛からシステムを解放する。

#### 3. ユーザー意識外での自律性 (Autonomy)
高度な技術は、ユーザーにその複雑さを意識させてはならない。
Driver API の `cuMemMap` や `cuMemSetAccess` は極めて低レイヤかつ複雑な操作を要求するが、本ライブラリはこれらを全て内部で隠蔽する。

* **断片化の自動解消**: ユーザーが意識せずとも、ライブラリが裏側で物理メモリの「継ぎ接ぎ（Scatter-Gather）」を行い、見かけ上の連続性を維持する。

* **OOMからの自動復旧**: メモリ不足発生時、ライブラリが自律的にガベージコレクション（Pool Drain）と再確保を行い、アプリケーションをクラッシュから守る。

ユーザー（開発者）は、これまで通り `cudaMalloc` を呼ぶだけでよい。その裏側で、OS レベルの高度なメモリ管理が自動的に遂行される世界観を目指す。
